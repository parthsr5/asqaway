{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1AiynTEXCNgNRae66pp7o0Xmo3AhfuLTB","timestamp":1680195861971}],"machine_shape":"hm","mount_file_id":"1RAatxv8sjfONv0J_2YUIMmy4JxkHhhRE","authorship_tag":"ABX9TyOdZJWm7W8qj5yN1w513GMT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!git clone https://github.com/huggingface/transformers.git \n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j-AL3PZDR6o7","executionInfo":{"status":"ok","timestamp":1682651093146,"user_tz":240,"elapsed":13922,"user":{"displayName":"Tanay Gummadi","userId":"01188912517462044987"}},"outputId":"052f1a28-6636-4f9c-d26a-7db94e43afcb"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 138186, done.\u001b[K\n","remote: Counting objects: 100% (1960/1960), done.\u001b[K\n","remote: Compressing objects: 100% (796/796), done.\u001b[K\n","remote: Total 138186 (delta 1241), reused 1605 (delta 1009), pack-reused 136226\u001b[K\n","Receiving objects: 100% (138186/138186), 137.46 MiB | 25.57 MiB/s, done.\n","Resolving deltas: 100% (103369/103369), done.\n","drive  sample_data  transformers\n"]}]},{"cell_type":"code","source":["%cd transformers/examples/pytorch/question-answering"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X6V_MeoERnd6","executionInfo":{"status":"ok","timestamp":1682651093147,"user_tz":240,"elapsed":6,"user":{"displayName":"Tanay Gummadi","userId":"01188912517462044987"}},"outputId":"9ad580f0-c56d-4813-ed4a-6713b3f8a596"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/transformers/examples/pytorch/question-answering\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUbTbKQMS5RA","executionInfo":{"status":"ok","timestamp":1682651107435,"user_tz":240,"elapsed":14292,"user":{"displayName":"Tanay Gummadi","userId":"01188912517462044987"}},"outputId":"b9e51729-eb9b-4e09-ec4f-05200019e52a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting accelerate>=0.12.0\n","  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=1.8.0\n","  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.0+cu118)\n","Collecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.5.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.65.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.27.1)\n","Collecting dill<0.3.7,>=0.3.0\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.4.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (9.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (2.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (3.12.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (3.1.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (3.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->-r requirements.txt (line 3)) (16.0.2)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->-r requirements.txt (line 3)) (3.25.2)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.12)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (23.1.0)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.26.15)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->-r requirements.txt (line 3)) (2.1.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2022.7.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->-r requirements.txt (line 3)) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (1.16.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets, evaluate, accelerate\n","Successfully installed accelerate-0.18.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install sentencepiece "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zLROg2mxTDMB","executionInfo":{"status":"ok","timestamp":1682651125806,"user_tz":240,"elapsed":18376,"user":{"displayName":"Tanay Gummadi","userId":"01188912517462044987"}},"outputId":"3463a151-b892-4923-ebe1-7bd463d6ae6f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Installing collected packages: tokenizers, transformers\n","Successfully installed tokenizers-0.13.3 transformers-4.28.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.98\n"]}]},{"cell_type":"code","source":["!python run_qa.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4mY7xiC5Qt7B","executionInfo":{"status":"ok","timestamp":1678733448976,"user_tz":240,"elapsed":16132,"user":{"displayName":"Tanay Gummadi","userId":"01188912517462044987"}},"outputId":"9b6ac8df-5647-4ddd-8d3a-9227665b668c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-13 18:50:36.110424: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-13 18:50:39.796385: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2023-03-13 18:50:39.802960: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2023-03-13 18:50:39.803013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","usage: run_qa.py\n","       [-h]\n","       --model_name_or_path\n","       MODEL_NAME_OR_PATH\n","       [--config_name CONFIG_NAME]\n","       [--tokenizer_name TOKENIZER_NAME]\n","       [--cache_dir CACHE_DIR]\n","       [--model_revision MODEL_REVISION]\n","       [--use_auth_token [USE_AUTH_TOKEN]]\n","       [--dataset_name DATASET_NAME]\n","       [--dataset_config_name DATASET_CONFIG_NAME]\n","       [--train_file TRAIN_FILE]\n","       [--validation_file VALIDATION_FILE]\n","       [--test_file TEST_FILE]\n","       [--overwrite_cache [OVERWRITE_CACHE]]\n","       [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n","       [--max_seq_length MAX_SEQ_LENGTH]\n","       [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n","       [--no_pad_to_max_length]\n","       [--max_train_samples MAX_TRAIN_SAMPLES]\n","       [--max_eval_samples MAX_EVAL_SAMPLES]\n","       [--max_predict_samples MAX_PREDICT_SAMPLES]\n","       [--version_2_with_negative [VERSION_2_WITH_NEGATIVE]]\n","       [--null_score_diff_threshold NULL_SCORE_DIFF_THRESHOLD]\n","       [--doc_stride DOC_STRIDE]\n","       [--n_best_size N_BEST_SIZE]\n","       [--max_answer_length MAX_ANSWER_LENGTH]\n","       --output_dir\n","       OUTPUT_DIR\n","       [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n","       [--do_train [DO_TRAIN]]\n","       [--do_eval [DO_EVAL]]\n","       [--do_predict [DO_PREDICT]]\n","       [--evaluation_strategy {no,steps,epoch}]\n","       [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n","       [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n","       [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n","       [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n","       [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n","       [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n","       [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n","       [--eval_delay EVAL_DELAY]\n","       [--learning_rate LEARNING_RATE]\n","       [--weight_decay WEIGHT_DECAY]\n","       [--adam_beta1 ADAM_BETA1]\n","       [--adam_beta2 ADAM_BETA2]\n","       [--adam_epsilon ADAM_EPSILON]\n","       [--max_grad_norm MAX_GRAD_NORM]\n","       [--num_train_epochs NUM_TRAIN_EPOCHS]\n","       [--max_steps MAX_STEPS]\n","       [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n","       [--warmup_ratio WARMUP_RATIO]\n","       [--warmup_steps WARMUP_STEPS]\n","       [--log_level {debug,info,warning,error,critical,passive}]\n","       [--log_level_replica {debug,info,warning,error,critical,passive}]\n","       [--log_on_each_node [LOG_ON_EACH_NODE]]\n","       [--no_log_on_each_node]\n","       [--logging_dir LOGGING_DIR]\n","       [--logging_strategy {no,steps,epoch}]\n","       [--logging_first_step [LOGGING_FIRST_STEP]]\n","       [--logging_steps LOGGING_STEPS]\n","       [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n","       [--no_logging_nan_inf_filter]\n","       [--save_strategy {no,steps,epoch}]\n","       [--save_steps SAVE_STEPS]\n","       [--save_total_limit SAVE_TOTAL_LIMIT]\n","       [--save_on_each_node [SAVE_ON_EACH_NODE]]\n","       [--no_cuda [NO_CUDA]]\n","       [--use_mps_device [USE_MPS_DEVICE]]\n","       [--seed SEED]\n","       [--data_seed DATA_SEED]\n","       [--jit_mode_eval [JIT_MODE_EVAL]]\n","       [--use_ipex [USE_IPEX]]\n","       [--bf16 [BF16]]\n","       [--fp16 [FP16]]\n","       [--fp16_opt_level FP16_OPT_LEVEL]\n","       [--half_precision_backend {auto,cuda_amp,apex,cpu_amp}]\n","       [--bf16_full_eval [BF16_FULL_EVAL]]\n","       [--fp16_full_eval [FP16_FULL_EVAL]]\n","       [--tf32 TF32]\n","       [--local_rank LOCAL_RANK]\n","       [--xpu_backend {mpi,ccl,gloo}]\n","       [--tpu_num_cores TPU_NUM_CORES]\n","       [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n","       [--debug DEBUG]\n","       [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n","       [--eval_steps EVAL_STEPS]\n","       [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n","       [--past_index PAST_INDEX]\n","       [--run_name RUN_NAME]\n","       [--disable_tqdm DISABLE_TQDM]\n","       [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n","       [--no_remove_unused_columns]\n","       [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n","       [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n","       [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n","       [--greater_is_better GREATER_IS_BETTER]\n","       [--ignore_data_skip [IGNORE_DATA_SKIP]]\n","       [--sharded_ddp SHARDED_DDP]\n","       [--fsdp FSDP]\n","       [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n","       [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n","       [--deepspeed DEEPSPEED]\n","       [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n","       [--optim {adamw_hf,adamw_torch,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_bnb_8bit,adamw_anyprecision,sgd,adagrad}]\n","       [--optim_args OPTIM_ARGS]\n","       [--adafactor [ADAFACTOR]]\n","       [--group_by_length [GROUP_BY_LENGTH]]\n","       [--length_column_name LENGTH_COLUMN_NAME]\n","       [--report_to REPORT_TO [REPORT_TO ...]]\n","       [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n","       [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n","       [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n","       [--no_dataloader_pin_memory]\n","       [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n","       [--no_skip_memory_metrics]\n","       [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n","       [--push_to_hub [PUSH_TO_HUB]]\n","       [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n","       [--hub_model_id HUB_MODEL_ID]\n","       [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n","       [--hub_token HUB_TOKEN]\n","       [--hub_private_repo [HUB_PRIVATE_REPO]]\n","       [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n","       [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n","       [--fp16_backend {auto,cuda_amp,apex,cpu_amp}]\n","       [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n","       [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n","       [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n","       [--mp_parameters MP_PARAMETERS]\n","       [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n","       [--full_determinism [FULL_DETERMINISM]]\n","       [--torchdynamo {eager,aot_eager,inductor,nvfuser,aot_nvfuser,aot_cudagraphs,ofi,fx2trt,onnxrt,ipex}]\n","       [--ray_scope RAY_SCOPE]\n","       [--ddp_timeout DDP_TIMEOUT]\n","       [--torch_compile [TORCH_COMPILE]]\n","       [--torch_compile_backend {eager,aot_eager,inductor,nvfuser,aot_nvfuser,aot_cudagraphs,ofi,fx2trt,onnxrt,ipex}]\n","       [--torch_compile_mode {default,reduce-overhead,max-autotune}]\n","run_qa.py: error: the following arguments are required: --model_name_or_path, --output_dir\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","    --model_name_or_path /content/drive/MyDrive/11797/biobert_base_11b_v2/pytorch_model.bin \\\n","    --config_name /content/drive/MyDrive/11797/biobert_base_11b_v2/config.json \\\n","    --tokenizer_name dmis-lab/biobert-base-cased-v1.1-squad \\\n","    --do_predict=True \\\n","    --test_file test3_file_modified.json \\\n","    --num_train_epochs 1 \\\n","    --save_strategy epoch \\\n","    --output_dir /content/biobert_base_11b_v2_test_3 \\\n","    --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WX9xrQNnT_yu","outputId":"84d2ba94-a6fd-4b5a-d08a-93f9a0f46861","executionInfo":{"status":"ok","timestamp":1682651745058,"user_tz":240,"elapsed":18629,"user":{"displayName":"Tanay Gummadi","userId":"01188912517462044987"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-04-28 03:15:29.213973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","04/28/2023 03:15:31 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/28/2023 03:15:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=False,\n","do_predict=True,\n","do_train=False,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=/content/biobert_base_11b_v2_test_3/runs/Apr28_03-15-31_3d6c5f963a8d,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=1.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=/content/biobert_base_11b_v2_test_3,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=8,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/biobert_base_11b_v2_test_3,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/28/2023 03:15:32 - INFO - datasets.builder - Using custom data configuration default-b0985b67889fe9db\n","04/28/2023 03:15:32 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","04/28/2023 03:15:32 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-b0985b67889fe9db/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n","Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b0985b67889fe9db/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...\n","Downloading data files: 100% 1/1 [00:00<00:00, 6472.69it/s]\n","04/28/2023 03:15:32 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","04/28/2023 03:15:32 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 1/1 [00:00<00:00, 1356.06it/s]\n","04/28/2023 03:15:32 - INFO - datasets.builder - Generating test split\n","04/28/2023 03:15:32 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b0985b67889fe9db/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e. Subsequent calls will reuse this data.\n","100% 1/1 [00:00<00:00, 716.24it/s]\n","[INFO|configuration_utils.py:666] 2023-04-28 03:15:32,148 >> loading configuration file /content/drive/MyDrive/11797/biobert_base_11b_v2/config.json\n","[INFO|configuration_utils.py:720] 2023-04-28 03:15:32,151 >> Model config BertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/11797/biobert_base_11b_v2/config.json\",\n","  \"architectures\": [\n","    \"BertForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_auto.py:502] 2023-04-28 03:15:32,361 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:668] 2023-04-28 03:15:32,553 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1-squad/snapshots/aa11b70199b5446ecf6ad5408f7b7a234686d7f2/config.json\n","[INFO|configuration_utils.py:720] 2023-04-28 03:15:32,554 >> Model config BertConfig {\n","  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1-squad\",\n","  \"architectures\": [\n","    \"BertForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:15:32,932 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1-squad/snapshots/aa11b70199b5446ecf6ad5408f7b7a234686d7f2/vocab.txt\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:15:32,932 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:15:32,932 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:15:32,932 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:15:32,932 >> loading file tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:668] 2023-04-28 03:15:32,933 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1-squad/snapshots/aa11b70199b5446ecf6ad5408f7b7a234686d7f2/config.json\n","[INFO|configuration_utils.py:720] 2023-04-28 03:15:32,933 >> Model config BertConfig {\n","  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1-squad\",\n","  \"architectures\": [\n","    \"BertForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:668] 2023-04-28 03:15:32,960 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1-squad/snapshots/aa11b70199b5446ecf6ad5408f7b7a234686d7f2/config.json\n","[INFO|configuration_utils.py:720] 2023-04-28 03:15:32,961 >> Model config BertConfig {\n","  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1-squad\",\n","  \"architectures\": [\n","    \"BertForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:2531] 2023-04-28 03:15:32,997 >> loading weights file /content/drive/MyDrive/11797/biobert_base_11b_v2/pytorch_model.bin\n","[INFO|modeling_utils.py:3190] 2023-04-28 03:15:34,407 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n","\n","[INFO|modeling_utils.py:3198] 2023-04-28 03:15:34,407 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /content/drive/MyDrive/11797/biobert_base_11b_v2/pytorch_model.bin.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n","Running tokenizer on prediction dataset:   0% 0/169 [00:00<?, ? examples/s]04/28/2023 03:15:34 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b0985b67889fe9db/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-8379b03276f8b8ac.arrow\n","04/28/2023 03:15:37 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:762] 2023-04-28 03:15:37,559 >> The following columns in the test set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3129] 2023-04-28 03:15:37,561 >> ***** Running Prediction *****\n","[INFO|trainer.py:3131] 2023-04-28 03:15:37,561 >>   Num examples = 169\n","[INFO|trainer.py:3134] 2023-04-28 03:15:37,561 >>   Batch size = 8\n"," 95% 21/22 [00:03<00:00,  5.52it/s]04/28/2023 03:15:42 - INFO - utils_qa - Post-processing 169 example predictions split into 169 features.\n","\n","  0% 0/169 [00:00<?, ?it/s]\u001b[A\n"," 22% 37/169 [00:00<00:00, 366.49it/s]\u001b[A\n"," 44% 74/169 [00:00<00:00, 360.07it/s]\u001b[A\n"," 66% 111/169 [00:00<00:00, 364.47it/s]\u001b[A\n","100% 169/169 [00:00<00:00, 363.76it/s]\n","04/28/2023 03:15:42 - INFO - utils_qa - Saving predictions to /content/biobert_base_11b_v2_test_3/predict_predictions.json.\n","04/28/2023 03:15:42 - INFO - utils_qa - Saving nbest_preds to /content/biobert_base_11b_v2_test_3/predict_nbest_predictions.json.\n","***** predict metrics *****\n","  predict_samples         =        169\n","  test_exact_match        =        0.0\n","  test_f1                 =        0.0\n","  test_runtime            = 0:00:04.44\n","  test_samples_per_second =     38.055\n","  test_steps_per_second   =      4.954\n","[INFO|modelcard.py:451] 2023-04-28 03:15:42,709 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n","100% 22/22 [00:04<00:00,  5.08it/s]\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","    --model_name_or_path /content/drive/MyDrive/11797/bioelectra_base_11b_v2/pytorch_model.bin \\\n","    --config_name /content/drive/MyDrive/11797/bioelectra_base_11b_v2/config.json \\\n","    --tokenizer_name sultan/BioM-ELECTRA-Base-SQuAD2 \\\n","    --do_predict=True \\\n","    --test_file test3_file_modified.json \\\n","    --num_train_epochs 1 \\\n","    --save_strategy epoch \\\n","    --output_dir /content/bioelectra_base_11b_v2_test_3 \\\n","    --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQrXK8cC3yGW","executionInfo":{"status":"ok","timestamp":1682651783188,"user_tz":240,"elapsed":32311,"user":{"displayName":"Tanay Gummadi","userId":"01188912517462044987"}},"outputId":"3acaf33f-5002-47c2-dc7b-69eda97856f1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-04-28 03:15:54.991329: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","04/28/2023 03:15:57 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/28/2023 03:15:57 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=False,\n","do_predict=True,\n","do_train=False,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=/content/bioelectra_base_11b_v2_test_3/runs/Apr28_03-15-57_3d6c5f963a8d,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=1.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=/content/bioelectra_base_11b_v2_test_3,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=8,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/bioelectra_base_11b_v2_test_3,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/28/2023 03:15:58 - INFO - datasets.builder - Using custom data configuration default-b0985b67889fe9db\n","04/28/2023 03:15:58 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","04/28/2023 03:15:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","04/28/2023 03:15:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b0985b67889fe9db/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n","04/28/2023 03:15:58 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b0985b67889fe9db/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n","04/28/2023 03:15:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b0985b67889fe9db/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n","100% 1/1 [00:00<00:00, 631.01it/s]\n","[INFO|configuration_utils.py:666] 2023-04-28 03:15:59,177 >> loading configuration file /content/drive/MyDrive/11797/bioelectra_base_11b_v2/config.json\n","[INFO|configuration_utils.py:720] 2023-04-28 03:15:59,181 >> Model config ElectraConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/11797/bioelectra_base_11b_v2/config.json\",\n","  \"architectures\": [\n","    \"ElectraForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"embedding_size\": 768,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"electra\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"summary_activation\": \"gelu\",\n","  \"summary_last_dropout\": 0.1,\n","  \"summary_type\": \"first\",\n","  \"summary_use_proj\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28895\n","}\n","\n","[INFO|tokenization_auto.py:502] 2023-04-28 03:15:59,397 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","Downloading (…)lve/main/config.json: 100% 761/761 [00:00<00:00, 3.74MB/s]\n","[INFO|configuration_utils.py:668] 2023-04-28 03:15:59,786 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-SQuAD2/snapshots/d94a78b570bf961f9500d7d8785689544fa6cfa7/config.json\n","[INFO|configuration_utils.py:720] 2023-04-28 03:15:59,787 >> Model config ElectraConfig {\n","  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-SQuAD2\",\n","  \"architectures\": [\n","    \"ElectraForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"embedding_size\": 768,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"electra\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"summary_activation\": \"gelu\",\n","  \"summary_last_dropout\": 0.1,\n","  \"summary_type\": \"first\",\n","  \"summary_use_proj\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28895\n","}\n","\n","Downloading (…)solve/main/vocab.txt: 100% 225k/225k [00:00<00:00, 23.1MB/s]\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:16:00,975 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-SQuAD2/snapshots/d94a78b570bf961f9500d7d8785689544fa6cfa7/vocab.txt\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:16:00,975 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:16:00,975 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:16:00,975 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:16:00,975 >> loading file tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:668] 2023-04-28 03:16:00,976 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-SQuAD2/snapshots/d94a78b570bf961f9500d7d8785689544fa6cfa7/config.json\n","[INFO|configuration_utils.py:720] 2023-04-28 03:16:00,976 >> Model config ElectraConfig {\n","  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-SQuAD2\",\n","  \"architectures\": [\n","    \"ElectraForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"embedding_size\": 768,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"electra\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"summary_activation\": \"gelu\",\n","  \"summary_last_dropout\": 0.1,\n","  \"summary_type\": \"first\",\n","  \"summary_use_proj\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28895\n","}\n","\n","[INFO|configuration_utils.py:668] 2023-04-28 03:16:01,002 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ELECTRA-Base-SQuAD2/snapshots/d94a78b570bf961f9500d7d8785689544fa6cfa7/config.json\n","[INFO|configuration_utils.py:720] 2023-04-28 03:16:01,003 >> Model config ElectraConfig {\n","  \"_name_or_path\": \"sultan/BioM-ELECTRA-Base-SQuAD2\",\n","  \"architectures\": [\n","    \"ElectraForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"embedding_size\": 768,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"electra\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"summary_activation\": \"gelu\",\n","  \"summary_last_dropout\": 0.1,\n","  \"summary_type\": \"first\",\n","  \"summary_use_proj\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28895\n","}\n","\n","[INFO|modeling_utils.py:2531] 2023-04-28 03:16:01,038 >> loading weights file /content/drive/MyDrive/11797/bioelectra_base_11b_v2/pytorch_model.bin\n","[INFO|modeling_utils.py:3190] 2023-04-28 03:16:12,448 >> All model checkpoint weights were used when initializing ElectraForQuestionAnswering.\n","\n","[INFO|modeling_utils.py:3198] 2023-04-28 03:16:12,448 >> All the weights of ElectraForQuestionAnswering were initialized from the model checkpoint at /content/drive/MyDrive/11797/bioelectra_base_11b_v2/pytorch_model.bin.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForQuestionAnswering for predictions without further training.\n","Running tokenizer on prediction dataset:   0% 0/169 [00:00<?, ? examples/s]04/28/2023 03:16:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b0985b67889fe9db/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-15860394f0682ae3.arrow\n","04/28/2023 03:16:15 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:762] 2023-04-28 03:16:15,594 >> The following columns in the test set don't have a corresponding argument in `ElectraForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `ElectraForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3129] 2023-04-28 03:16:15,596 >> ***** Running Prediction *****\n","[INFO|trainer.py:3131] 2023-04-28 03:16:15,596 >>   Num examples = 169\n","[INFO|trainer.py:3134] 2023-04-28 03:16:15,596 >>   Batch size = 8\n"," 95% 21/22 [00:03<00:00,  5.54it/s]04/28/2023 03:16:20 - INFO - utils_qa - Post-processing 169 example predictions split into 169 features.\n","\n","  0% 0/169 [00:00<?, ?it/s]\u001b[A\n"," 23% 39/169 [00:00<00:00, 383.01it/s]\u001b[A\n"," 46% 78/169 [00:00<00:00, 384.03it/s]\u001b[A\n"," 70% 118/169 [00:00<00:00, 389.28it/s]\u001b[A\n","100% 169/169 [00:00<00:00, 386.17it/s]\n","04/28/2023 03:16:20 - INFO - utils_qa - Saving predictions to /content/bioelectra_base_11b_v2_test_3/predict_predictions.json.\n","04/28/2023 03:16:20 - INFO - utils_qa - Saving nbest_preds to /content/bioelectra_base_11b_v2_test_3/predict_nbest_predictions.json.\n","***** predict metrics *****\n","  predict_samples         =        169\n","  test_exact_match        =        0.0\n","  test_f1                 =        0.0\n","  test_runtime            = 0:00:04.41\n","  test_samples_per_second =     38.276\n","  test_steps_per_second   =      4.983\n","[INFO|modelcard.py:451] 2023-04-28 03:16:21,150 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n","100% 22/22 [00:04<00:00,  4.60it/s]\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","   --model_name_or_path /content/drive/MyDrive/11797/bioalbert_base_11b_v2/pytorch_model.bin \\\n","    --config_name /content/drive/MyDrive/11797/bioalbert_base_11b_v2/config.json \\\n","    --tokenizer_name sultan/BioM-ALBERT-xxlarge-SQuAD2 \\\n","    --do_predict=True \\\n","    --test_file test3_file_modified.json \\\n","    --num_train_epochs 1 \\\n","    --save_strategy epoch \\\n","    --output_dir /content/bioalbert_base_11b_v2_test_3 \\\n","    --fp16 \\\n","    --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQLGBsEU_UgW","executionInfo":{"status":"ok","timestamp":1682651838970,"user_tz":240,"elapsed":55795,"user":{"displayName":"Tanay Gummadi","userId":"01188912517462044987"}},"outputId":"0b87a28b-9173-4f23-cd55-be8bc1aecd89"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-04-28 03:16:26.933234: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","04/28/2023 03:16:29 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n","04/28/2023 03:16:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=False,\n","do_predict=True,\n","do_train=False,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=True,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=/content/bioalbert_base_11b_v2_test_3/runs/Apr28_03-16-28_3d6c5f963a8d,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=1.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=/content/bioalbert_base_11b_v2_test_3,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=8,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/bioalbert_base_11b_v2_test_3,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=500,\n","save_strategy=epoch,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/28/2023 03:16:30 - INFO - datasets.builder - Using custom data configuration default-b0985b67889fe9db\n","04/28/2023 03:16:30 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","04/28/2023 03:16:30 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","04/28/2023 03:16:30 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b0985b67889fe9db/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n","04/28/2023 03:16:30 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b0985b67889fe9db/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n","04/28/2023 03:16:30 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-b0985b67889fe9db/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n","100% 1/1 [00:00<00:00, 632.82it/s]\n","[INFO|configuration_utils.py:666] 2023-04-28 03:16:31,214 >> loading configuration file /content/drive/MyDrive/11797/bioalbert_base_11b_v2/config.json\n","[INFO|configuration_utils.py:720] 2023-04-28 03:16:31,217 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/11797/bioalbert_base_11b_v2/config.json\",\n","  \"architectures\": [\n","    \"AlbertForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.01,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 16384,\n","  \"layer_norm_eps\": 1e-12,\n","  \"layers_to_keep\": [],\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 64,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","Downloading (…)okenizer_config.json: 100% 85.0/85.0 [00:00<00:00, 530kB/s]\n","Downloading (…)lve/main/config.json: 100% 715/715 [00:00<00:00, 4.25MB/s]\n","[INFO|configuration_utils.py:668] 2023-04-28 03:16:32,055 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-SQuAD2/snapshots/71d586c571c68eaad6e1c994b557a2b1643f7e1d/config.json\n","[INFO|configuration_utils.py:720] 2023-04-28 03:16:32,056 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-SQuAD2\",\n","  \"architectures\": [\n","    \"AlbertForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.01,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 16384,\n","  \"layer_norm_eps\": 1e-12,\n","  \"layers_to_keep\": [],\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 64,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","Downloading spiece.model: 100% 778k/778k [00:00<00:00, 895kB/s]\n","Downloading (…)cial_tokens_map.json: 100% 156/156 [00:00<00:00, 971kB/s]\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:16:34,826 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-SQuAD2/snapshots/71d586c571c68eaad6e1c994b557a2b1643f7e1d/spiece.model\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:16:34,826 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:16:34,826 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:16:34,826 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-SQuAD2/snapshots/71d586c571c68eaad6e1c994b557a2b1643f7e1d/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:1809] 2023-04-28 03:16:34,826 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-SQuAD2/snapshots/71d586c571c68eaad6e1c994b557a2b1643f7e1d/tokenizer_config.json\n","[INFO|configuration_utils.py:668] 2023-04-28 03:16:34,827 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-SQuAD2/snapshots/71d586c571c68eaad6e1c994b557a2b1643f7e1d/config.json\n","[INFO|configuration_utils.py:720] 2023-04-28 03:16:34,827 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-SQuAD2\",\n","  \"architectures\": [\n","    \"AlbertForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.01,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 16384,\n","  \"layer_norm_eps\": 1e-12,\n","  \"layers_to_keep\": [],\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 64,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|configuration_utils.py:668] 2023-04-28 03:16:34,881 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sultan--BioM-ALBERT-xxlarge-SQuAD2/snapshots/71d586c571c68eaad6e1c994b557a2b1643f7e1d/config.json\n","[INFO|configuration_utils.py:720] 2023-04-28 03:16:34,881 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"sultan/BioM-ALBERT-xxlarge-SQuAD2\",\n","  \"architectures\": [\n","    \"AlbertForQuestionAnswering\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.01,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 16384,\n","  \"layer_norm_eps\": 1e-12,\n","  \"layers_to_keep\": [],\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 64,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.28.1\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|modeling_utils.py:2531] 2023-04-28 03:16:34,996 >> loading weights file /content/drive/MyDrive/11797/bioalbert_base_11b_v2/pytorch_model.bin\n","[INFO|modeling_utils.py:3190] 2023-04-28 03:16:56,623 >> All model checkpoint weights were used when initializing AlbertForQuestionAnswering.\n","\n","[INFO|modeling_utils.py:3198] 2023-04-28 03:16:56,624 >> All the weights of AlbertForQuestionAnswering were initialized from the model checkpoint at /content/drive/MyDrive/11797/bioalbert_base_11b_v2/pytorch_model.bin.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForQuestionAnswering for predictions without further training.\n","Running tokenizer on prediction dataset:   0% 0/169 [00:00<?, ? examples/s]04/28/2023 03:16:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-b0985b67889fe9db/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-9e05185a4f3be2ed.arrow\n","[INFO|trainer.py:621] 2023-04-28 03:16:59,937 >> Using cuda_amp half precision backend\n","04/28/2023 03:16:59 - INFO - __main__ - *** Predict ***\n","[INFO|trainer.py:762] 2023-04-28 03:16:59,937 >> The following columns in the test set don't have a corresponding argument in `AlbertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `AlbertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:3129] 2023-04-28 03:16:59,939 >> ***** Running Prediction *****\n","[INFO|trainer.py:3131] 2023-04-28 03:16:59,939 >>   Num examples = 169\n","[INFO|trainer.py:3134] 2023-04-28 03:16:59,939 >>   Batch size = 8\n","100% 22/22 [00:14<00:00,  1.79it/s]04/28/2023 03:17:16 - INFO - utils_qa - Post-processing 169 example predictions split into 169 features.\n","\n","  0% 0/169 [00:00<?, ?it/s]\u001b[A\n"," 24% 40/169 [00:00<00:00, 391.74it/s]\u001b[A\n"," 47% 80/169 [00:00<00:00, 395.33it/s]\u001b[A\n"," 71% 120/169 [00:00<00:00, 396.69it/s]\u001b[A\n","100% 169/169 [00:00<00:00, 392.98it/s]\n","04/28/2023 03:17:16 - INFO - utils_qa - Saving predictions to /content/bioalbert_base_11b_v2_test_3/predict_predictions.json.\n","04/28/2023 03:17:16 - INFO - utils_qa - Saving nbest_preds to /content/bioalbert_base_11b_v2_test_3/predict_nbest_predictions.json.\n","***** predict metrics *****\n","  predict_samples         =        169\n","  test_exact_match        =        0.0\n","  test_f1                 =        0.0\n","  test_runtime            = 0:00:16.36\n","  test_samples_per_second =     10.326\n","  test_steps_per_second   =      1.344\n","[INFO|modelcard.py:451] 2023-04-28 03:17:16,960 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n","100% 22/22 [00:15<00:00,  1.41it/s]\n"]}]},{"cell_type":"code","source":["%cd /content\n","!ls\n","!mv biobert_base_11b_v2_test_3/ drive/MyDrive/11797\n","!mv bioelectra_base_11b_v2_test_3/ drive/MyDrive/11797\n","!mv bioalbert_base_11b_v2_test_3/ drive/MyDrive/11797"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nMqLNcTtR5Bo","executionInfo":{"status":"ok","timestamp":1682651847574,"user_tz":240,"elapsed":1164,"user":{"displayName":"Tanay Gummadi","userId":"01188912517462044987"}},"outputId":"95a57cb8-9689-4715-86ca-841b3697f048"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","bioalbert_base_11b_v2_test_3  bioelectra_base_11b_v2_test_3  sample_data\n","biobert_base_11b_v2_test_3    drive\t\t\t     transformers\n"]}]}]}